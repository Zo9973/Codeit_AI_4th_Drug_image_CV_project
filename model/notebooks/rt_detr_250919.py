import os
import json
import glob
import pandas as pd
from pathlib import Path
import cv2
import numpy as np
from ultralytics import RTDETR
import yaml
import shutil
from collections import defaultdict
from datetime import datetime
import copy

"""## ì „ì²˜ë¦¬ í´ë˜ìŠ¤"""

class RTDETRDataProcessor:
    def __init__(self, data_path="./data"):
        self.data_path = data_path
        self.train_images_path = os.path.join(data_path, "train_images")
        self.train_ann_path = os.path.join(data_path, "train_annotations")
        self.test_images_path = os.path.join(data_path, "test_images")

        # YOLO í˜•ì‹ ì¶œë ¥ ë””ë ‰í† ë¦¬
        self.output_dir = os.path.join(data_path, "yolo_format")
        os.makedirs(self.output_dir, exist_ok=True)

    def merge_coco_annotations(self):
        """COCO í˜•ì‹ JSON íŒŒì¼ë“¤ì„ í•˜ë‚˜ë¡œ ë³‘í•© + ëª¨ë“  dl_idx ë§¤í•‘ ì €ì¥ (ìˆ˜ì •ëœ ë²„ì „)"""
        pattern = os.path.join(self.train_ann_path, "**", "*.json")
        json_files = glob.glob(pattern, recursive=True)

        merged_data = {
            "images": [],
            "annotations": [],
            "categories": []
        }

        # ID ì¬í• ë‹¹ì„ ìœ„í•œ ë§¤í•‘
        image_id_map = {}
        category_name_to_id = {}
        new_image_id = 0
        new_ann_id = 0
        new_cat_id = 0

        # ğŸ”¥ í•µì‹¬: ëª¨ë“  ë§¤í•‘ì„ ì €ì¥í•˜ëŠ” êµ¬ì¡°
        dl_idx_to_name_mapping = {}    # ëª¨ë“  dl_idx â†’ ì•½í’ˆëª…
        category_mapping = {}          # YOLO í´ë˜ìŠ¤ ID â†’ dl_idx
        processed_dl_idx = set()       # ì¤‘ë³µ ë°©ì§€

        print(f"ì´ {len(json_files)}ê°œì˜ JSON íŒŒì¼ ì²˜ë¦¬ ì¤‘...")

        for json_file in json_files:
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                # íŒŒì¼ í˜•ì‹ í™•ì¸
                categories = data.get('categories', [])
                is_broken_format = (len(categories) == 1 and
                                categories[0].get('name') == 'Drug' and
                                categories[0].get('id') == 1)

                if is_broken_format:
                    print(f"  ìƒˆë¡œìš´ í˜•ì‹: {os.path.basename(json_file)}")

                    # ìƒˆë¡œìš´ í˜•ì‹: ì´ë¯¸ì§€ì—ì„œ dl_idxì™€ dl_name ì¶”ì¶œ
                    for img in data.get('images', []):
                        actual_dl_idx = img.get('dl_idx')
                        actual_dl_name = img.get('dl_name')

                        if actual_dl_idx and actual_dl_name:
                            if isinstance(actual_dl_idx, str):
                                actual_dl_idx = int(actual_dl_idx)

                            if actual_dl_idx not in processed_dl_idx:
                                processed_dl_idx.add(actual_dl_idx)

                                # ğŸ”¥ ìˆ˜ì •: ëª¨ë“  ë§¤í•‘ ì €ì¥
                                dl_idx_to_name_mapping[actual_dl_idx] = actual_dl_name

                                if actual_dl_name not in category_name_to_id:
                                    category_name_to_id[actual_dl_name] = new_cat_id
                                    category_mapping[str(new_cat_id)] = actual_dl_idx

                                    merged_data['categories'].append({
                                        'id': new_cat_id,
                                        'name': actual_dl_name,
                                        'supercategory': 'pill',
                                        'original_dl_idx': actual_dl_idx
                                    })

                                    print(f"    + {actual_dl_name} (dl_idx: {actual_dl_idx})")
                                    new_cat_id += 1

                else:
                    print(f"  ê¸°ì¡´ í˜•ì‹: {os.path.basename(json_file)}")

                    # ğŸ”¥ ìˆ˜ì •: ê¸°ì¡´ í˜•ì‹ë„ ë§¤í•‘ ì €ì¥
                    for cat in categories:
                        cat_name = cat['name']
                        original_dl_idx = cat['id']

                        if original_dl_idx not in processed_dl_idx:
                            processed_dl_idx.add(original_dl_idx)

                            # ğŸ”¥ í•µì‹¬: ê¸°ì¡´ ë°ì´í„°ë„ ë§¤í•‘ ì €ì¥
                            dl_idx_to_name_mapping[original_dl_idx] = cat_name

                            if cat_name not in category_name_to_id:
                                category_name_to_id[cat_name] = new_cat_id
                                category_mapping[str(new_cat_id)] = original_dl_idx

                                merged_data['categories'].append({
                                    'id': new_cat_id,
                                    'name': cat_name,
                                    'supercategory': 'pill',
                                    'original_dl_idx': original_dl_idx
                                })

                                print(f"    + {cat_name} (dl_idx: {original_dl_idx})")
                                new_cat_id += 1

                # ì´ë¯¸ì§€ ì²˜ë¦¬ (ê¸°ì¡´ê³¼ ë™ì¼)
                for img in data.get('images', []):
                    old_img_id = img['id']
                    image_id_map[old_img_id] = new_image_id
                    img['id'] = new_image_id
                    merged_data['images'].append(img)
                    new_image_id += 1

                # ì–´ë…¸í…Œì´ì…˜ ì²˜ë¦¬ (ìˆ˜ì •ë¨)
                for ann in data.get('annotations', []):
                    ann['id'] = new_ann_id
                    ann['image_id'] = image_id_map[ann['image_id']]

                    if is_broken_format:
                        # ìƒˆë¡œìš´ í˜•ì‹: ì´ë¯¸ì§€ì—ì„œ dl_idx ì°¾ê¸°
                        original_img_id = None
                        for orig_id, mapped_id in image_id_map.items():
                            if mapped_id == ann['image_id']:
                                original_img_id = orig_id
                                break

                        target_img = next((img for img in data['images'] if img['id'] == original_img_id), None)
                        if target_img and target_img.get('dl_idx'):
                            actual_dl_idx = int(target_img['dl_idx']) if isinstance(target_img['dl_idx'], str) else target_img['dl_idx']
                            actual_dl_name = target_img.get('dl_name')

                            if actual_dl_name in category_name_to_id:
                                ann['category_id'] = category_name_to_id[actual_dl_name]
                                merged_data['annotations'].append(ann)
                                new_ann_id += 1
                    else:
                        # ê¸°ì¡´ í˜•ì‹
                        old_cat_id = ann['category_id']
                        target_cat = next((cat for cat in data['categories'] if cat['id'] == old_cat_id), None)

                        if target_cat and target_cat['name'] in category_name_to_id:
                            ann['category_id'] = category_name_to_id[target_cat['name']]
                            merged_data['annotations'].append(ann)
                            new_ann_id += 1

            except Exception as e:
                print(f"âŒ ì—ëŸ¬: {json_file} - {e}")
                continue

        # ğŸ”¥ ìˆ˜ì •: ëª¨ë“  ë§¤í•‘ íŒŒì¼ ì €ì¥
        print(f"\nğŸ“ ë§¤í•‘ íŒŒì¼ ì €ì¥ ì¤‘...")

        # category_id_mapping.json (YOLO í´ë˜ìŠ¤ â†’ dl_idx)
        with open(os.path.join(self.output_dir, 'category_id_mapping.json'), 'w', encoding='utf-8') as f:
            json.dump(category_mapping, f, ensure_ascii=False, indent=2)

        # dl_idx_to_name.json (dl_idx â†’ ì•½í’ˆëª…)
        with open(os.path.join(self.output_dir, 'dl_idx_to_name.json'), 'w', encoding='utf-8') as f:
            json.dump(dl_idx_to_name_mapping, f, ensure_ascii=False, indent=2)

        # dl_name_mapping.json (ë™ì¼í•œ ë‚´ìš©ì´ì§€ë§Œ í˜¸í™˜ì„±ì„ ìœ„í•´)
        with open(os.path.join(self.output_dir, 'dl_name_mapping.json'), 'w', encoding='utf-8') as f:
            json.dump(dl_idx_to_name_mapping, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ¯ ë³‘í•© ê²°ê³¼:")
        print(f"  ğŸ“Š ì´ ì¹´í…Œê³ ë¦¬: {len(merged_data['categories'])}ê°œ")
        print(f"  ğŸ“Š ì´ ì´ë¯¸ì§€: {len(merged_data['images'])}ê°œ")
        print(f"  ğŸ“Š ì´ ì–´ë…¸í…Œì´ì…˜: {len(merged_data['annotations'])}ê°œ")
        print(f"  ğŸ“Š ë§¤í•‘ëœ dl_idx: {len(dl_idx_to_name_mapping)}ê°œ")

        # ë§¤í•‘ ê²€ì¦
        if len(dl_idx_to_name_mapping) != len(merged_data['categories']):
            print(f"  âš ï¸  ë§¤í•‘ ë¶ˆì¼ì¹˜ ê°ì§€!")
            print(f"     ì¹´í…Œê³ ë¦¬: {len(merged_data['categories'])}ê°œ")
            print(f"     ë§¤í•‘: {len(dl_idx_to_name_mapping)}ê°œ")
        else:
            print(f"  âœ… ë§¤í•‘ ì¼ì¹˜ í™•ì¸")

        return merged_data

    def _find_actual_drug_name(self, dl_idx, data, dl_name_map):
        """dl_idxë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹¤ì œ ì•½í’ˆëª… ì°¾ê¸°"""

        # ë°©ë²• 1: í•´ë‹¹ dl_idxë¥¼ ê°€ì§„ ì´ë¯¸ì§€ë“¤ì˜ dl_name í™•ì¸
        for img in data.get('images', []):
            if img.get('dl_name') and 'annotations' in data:
                # ì´ ì´ë¯¸ì§€ì˜ ì–´ë…¸í…Œì´ì…˜ ì¤‘ í•´ë‹¹ dl_idxë¥¼ ê°€ì§„ ê²ƒ ì°¾ê¸°
                for ann in data['annotations']:
                    if ann['image_id'] == img['id'] and ann['category_id'] == dl_idx:
                        return img.get('dl_name')

        # ë°©ë²• 2: dl_name_mapì—ì„œ ì§ì ‘ ì°¾ê¸°
        if dl_name_map:
            for filename, drug_name in dl_name_map.items():
                # íŒŒì¼ëª… íŒ¨í„´ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„
                if str(dl_idx) in filename:
                    return drug_name

        return None  # ì°¾ì§€ ëª»í•œ ê²½ìš°

    def validate_category_mapping(self):
        """ì¹´í…Œê³ ë¦¬ ë§¤í•‘ ê²€ì¦"""
        mapping_file = os.path.join(self.output_dir, 'category_id_mapping.json')
        dl_name_file = os.path.join(self.output_dir, 'dl_name_mapping.json')

        if os.path.exists(mapping_file) and os.path.exists(dl_name_file):
            with open(mapping_file, 'r') as f:
                category_mapping = json.load(f)
            with open(dl_name_file, 'r') as f:
                dl_name_mapping = json.load(f)

            print(f"\nğŸ” ë§¤í•‘ ê²€ì¦:")
            print(f"  - ì´ ì¹´í…Œê³ ë¦¬: {len(category_mapping)}ê°œ")
            print(f"  - dl_name ë§¤í•‘: {len(dl_name_mapping)}ê°œ")

            # "Drug"ê°€ ë‚¨ì•„ìˆëŠ”ì§€ í™•ì¸
            generic_drugs = [name for name in dl_name_mapping.values() if "Drug_idx_" in name]
            if generic_drugs:
                print(f"  âš ï¸  ë°±ì—…ëª… ì‚¬ìš©: {len(generic_drugs)}ê°œ")
                for drug in generic_drugs[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                    print(f"      {drug}")
            else:
                print(f"  âœ… ëª¨ë“  ì•½í’ˆëª… ë§¤í•‘ ì™„ë£Œ")

        return True

    def coco_to_yolo_format(self, merged_data):
        """COCO í˜•ì‹ì„ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ + ë§¤í•‘ íŒŒì¼ ìƒì„±"""
        # ë””ë ‰í† ë¦¬ ìƒì„±
        train_dir = os.path.join(self.output_dir, "train")
        val_dir = os.path.join(self.output_dir, "val")
        os.makedirs(os.path.join(train_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(train_dir, "labels"), exist_ok=True)
        os.makedirs(os.path.join(val_dir, "images"), exist_ok=True)
        os.makedirs(os.path.join(val_dir, "labels"), exist_ok=True)

        # ì´ë¯¸ì§€ ë¶„í•  (80:20)
        images = merged_data['images']
        np.random.seed(42)
        np.random.shuffle(images)
        split_idx = int(len(images) * 0.8)
        train_images = images[:split_idx]
        val_images = images[split_idx:]

        # ì–´ë…¸í…Œì´ì…˜ì„ ì´ë¯¸ì§€ë³„ë¡œ ê·¸ë£¹í™”
        ann_by_image = {}
        for ann in merged_data['annotations']:
            img_id = ann['image_id']
            if img_id not in ann_by_image:
                ann_by_image[img_id] = []
            ann_by_image[img_id].append(ann)

        # í›ˆë ¨ ë°ì´í„° ì²˜ë¦¬
        self._process_split(train_images, ann_by_image, train_dir, "train")

        # ê²€ì¦ ë°ì´í„° ì²˜ë¦¬
        self._process_split(val_images, ann_by_image, val_dir, "val")

        # ğŸ”¥ í•µì‹¬: category_id ë§¤í•‘ íŒŒì¼ ìƒì„± (ì œì¶œìš©)
        category_mapping = {}
        original_dl_idx_mapping = {}

        for i, cat in enumerate(merged_data['categories']):
            # YOLO í›ˆë ¨ìš© ID (0,1,2,3...)
            yolo_class_id = i

            # ì‹¤ì œ dl_idx ì¶”ì¶œ (ì›ë³¸ JSONì—ì„œ ë³´ì¡´ëœ ê°’)
            original_dl_idx = cat.get('original_dl_idx', cat['id'])

            category_mapping[str(yolo_class_id)] = original_dl_idx
            original_dl_idx_mapping[original_dl_idx] = cat['name']

        # ë§¤í•‘ íŒŒì¼ë“¤ ì €ì¥
        with open(os.path.join(self.output_dir, 'category_id_mapping.json'), 'w', encoding='utf-8') as f:
            json.dump(category_mapping, f, ensure_ascii=False, indent=2)

        with open(os.path.join(self.output_dir, 'dl_idx_to_name.json'), 'w', encoding='utf-8') as f:
            json.dump(original_dl_idx_mapping, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ”¥ ë§¤í•‘ íŒŒì¼ ìƒì„± ì™„ë£Œ:")
        print(f"  - category_id_mapping.json: YOLO í´ë˜ìŠ¤ â†’ dl_idx ë§¤í•‘")
        print(f"  - dl_idx_to_name.json: dl_idx â†’ ì•½í’ˆëª… ë§¤í•‘")
        print(f"  - ì´ {len(category_mapping)}ê°œ í´ë˜ìŠ¤")

        # ë§¤í•‘ ì˜ˆì‹œ ì¶œë ¥
        print(f"ë§¤í•‘ ì˜ˆì‹œ:")
        for yolo_id, dl_idx in list(category_mapping.items())[:5]:
            drug_name = original_dl_idx_mapping.get(dl_idx, "Unknown")
            print(f"  YOLO í´ë˜ìŠ¤ {yolo_id} â†’ dl_idx {dl_idx} ({drug_name})")

        # dataset.yaml íŒŒì¼ ìƒì„±
        yaml_data = {
            'path': self.output_dir,
            'train': 'train/images',
            'val': 'val/images',
            'nc': len(merged_data['categories']),
            'names': [cat['name'] for cat in merged_data['categories']]
        }

        with open(os.path.join(self.output_dir, 'dataset.yaml'), 'w', encoding='utf-8') as f:
            yaml.dump(yaml_data, f, allow_unicode=True)

        return merged_data

    def _process_split(self, images, ann_by_image, output_dir, split_name):
        """ë¶„í• ëœ ë°ì´í„° ì²˜ë¦¬ (bbox ìœ íš¨ì„± ê²€ì‚¬ í¬í•¨)"""
        skipped_annotations = 0
        processed_annotations = 0

        for img_data in images:
            img_id = img_data['id']
            filename = img_data['file_name']

            # ì´ë¯¸ì§€ ë³µì‚¬
            src_path = os.path.join(self.train_images_path, filename)
            dst_path = os.path.join(output_dir, "images", filename)
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)

            # YOLO ë¼ë²¨ ìƒì„±
            label_filename = filename.replace('.png', '.txt').replace('.jpg', '.txt')
            label_path = os.path.join(output_dir, "labels", label_filename)

            with open(label_path, 'w') as f:
                if img_id in ann_by_image:
                    for ann in ann_by_image[img_id]:
                        # ğŸ”¥ bbox ìœ íš¨ì„± ê²€ì‚¬ ì¶”ê°€
                        bbox = ann.get('bbox', [])
                        if not bbox or len(bbox) != 4:
                            print(f"âš ï¸ ì˜ëª»ëœ bbox ê±´ë„ˆë›°ê¸°: {bbox} (ì´ë¯¸ì§€: {filename})")
                            skipped_annotations += 1
                            continue

                        # COCO bboxë¥¼ YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                        x, y, w, h = bbox

                        # ğŸ”¥ ì¶”ê°€ ìœ íš¨ì„± ê²€ì‚¬ (ìŒìˆ˜ í¬ê¸° ë°©ì§€)
                        if w <= 0 or h <= 0:
                            print(f"âš ï¸ ìœ íš¨í•˜ì§€ ì•Šì€ í¬ê¸° ê±´ë„ˆë›°ê¸°: w={w}, h={h} (ì´ë¯¸ì§€: {filename})")
                            skipped_annotations += 1
                            continue

                        # ì´ë¯¸ì§€ í¬ê¸° ì •ë³´
                        img_w, img_h = img_data['width'], img_data['height']

                        # ğŸ”¥ ì´ë¯¸ì§€ ê²½ê³„ ì²´í¬
                        if x < 0 or y < 0 or (x + w) > img_w or (y + h) > img_h:
                            print(f"âš ï¸ ì´ë¯¸ì§€ ê²½ê³„ ì´ˆê³¼ bbox ê±´ë„ˆë›°ê¸°: ({x}, {y}, {w}, {h}) (ì´ë¯¸ì§€: {filename})")
                            skipped_annotations += 1
                            continue

                        # ì •ê·œí™”ëœ ì¤‘ì‹¬ì  ì¢Œí‘œì™€ í¬ê¸°
                        x_center = (x + w/2) / img_w
                        y_center = (y + h/2) / img_h
                        norm_w = w / img_w
                        norm_h = h / img_h

                        # ğŸ”¥ ì •ê·œí™”ëœ ê°’ ë²”ìœ„ ì²´í¬ (0~1 ì‚¬ì´)
                        if not (0 <= x_center <= 1 and 0 <= y_center <= 1 and 0 <= norm_w <= 1 and 0 <= norm_h <= 1):
                            print(f"âš ï¸ ì •ê·œí™” ê°’ ë²”ìœ„ ì´ˆê³¼: center=({x_center:.3f}, {y_center:.3f}), size=({norm_w:.3f}, {norm_h:.3f})")
                            skipped_annotations += 1
                            continue

                        class_id = ann['category_id']
                        f.write(f"{class_id} {x_center:.6f} {y_center:.6f} {norm_w:.6f} {norm_h:.6f}\n")
                        processed_annotations += 1

        # ğŸ”¥ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š {split_name} ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"  - ì²˜ë¦¬ëœ ì–´ë…¸í…Œì´ì…˜: {processed_annotations}ê°œ")
        print(f"  - ê±´ë„ˆë›´ ì–´ë…¸í…Œì´ì…˜: {skipped_annotations}ê°œ")
        if skipped_annotations > 0:
            print(f"  - ì„±ê³µë¥ : {processed_annotations/(processed_annotations + skipped_annotations)*100:.1f}%")

    def process_data_with_name_mapping(self):
        """dl_name ë§¤í•‘ í¬í•¨ ì „ì²´ ë°ì´í„° ì²˜ë¦¬"""
        print("=== dl_name ë§¤í•‘ í¬í•¨ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘ ===")

        print("1. COCO ì–´ë…¸í…Œì´ì…˜ ë³‘í•© + dl_name ë§¤í•‘ ì¤‘...")
        merged_data = self.merge_coco_annotations()  # ê°œì„ ëœ ë²„ì „

        print("2. ë§¤í•‘ ê²€ì¦ ì¤‘...")
        self.validate_category_mapping()

        print("3. YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        self.coco_to_yolo_format(merged_data)

        print("âœ… ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ (dl_name ë§¤í•‘ í¬í•¨)!")
        return merged_data

    def quick_mapping_check(self):
        """ë¹ ë¥¸ ë§¤í•‘ ìƒíƒœ í™•ì¸"""
        mapping_files = [
            'category_id_mapping.json',
            'dl_name_mapping.json',
            'dl_idx_to_name.json'
        ]

        print("ğŸ” ë§¤í•‘ íŒŒì¼ ìƒíƒœ:")
        all_exist = True
        for filename in mapping_files:
            filepath = os.path.join(self.output_dir, filename)
            if os.path.exists(filepath):
                with open(filepath, 'r') as f:
                    data = json.load(f)
                print(f"  âœ… {filename}: {len(data)}ê°œ í•­ëª©")
            else:
                print(f"  âŒ {filename}: ì—†ìŒ")
                all_exist = False

        return all_exist

    def process_data(self):
        """ì „ì²´ ë°ì´í„° ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ + í´ë˜ìŠ¤ ë¶ˆê· í˜• í•´ê²°"""
        print("COCO ì–´ë…¸í…Œì´ì…˜ ë³‘í•© ì¤‘...")
        merged_data = self.merge_coco_annotations()

        print("YOLO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")
        self.coco_to_yolo_format(merged_data)
        print("ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ!")

        return merged_data

    def analyze_class_distribution(self, merged_data):
        """í´ë˜ìŠ¤ë³„ ë°ì´í„° ë¶„í¬ ë¶„ì„"""
        print("=== í´ë˜ìŠ¤ ë¶„í¬ ë¶„ì„ ===")

        # í´ë˜ìŠ¤ë³„ ì´ë¯¸ì§€ ìˆ˜ ê³„ì‚°
        class_image_count = defaultdict(int)
        class_annotation_count = defaultdict(int)

        # ì´ë¯¸ì§€ë³„ í´ë˜ìŠ¤ ì§‘ê³„
        ann_by_image = {}
        for ann in merged_data['annotations']:
            img_id = ann['image_id']
            if img_id not in ann_by_image:
                ann_by_image[img_id] = set()
            ann_by_image[img_id].add(ann['category_id'])
            class_annotation_count[ann['category_id']] += 1

        # ê° í´ë˜ìŠ¤ê°€ í¬í•¨ëœ ì´ë¯¸ì§€ ìˆ˜
        for img_id, classes in ann_by_image.items():
            for class_id in classes:
                class_image_count[class_id] += 1

        # í´ë˜ìŠ¤ ì´ë¦„ ë§¤í•‘
        class_names = {cat['id']: cat['name'] for cat in merged_data['categories']}

        # ë¶„í¬ ì¶œë ¥
        print(f"ì´ {len(merged_data['categories'])}ê°œ í´ë˜ìŠ¤")
        print("\ní´ë˜ìŠ¤ë³„ ë¶„í¬ (ì–´ë…¸í…Œì´ì…˜ ìˆ˜ ê¸°ì¤€):")

        sorted_classes = sorted(class_annotation_count.items(), key=lambda x: x[1], reverse=True)

        minority_classes = []
        majority_classes = []

        for class_id, ann_count in sorted_classes:
            img_count = class_image_count[class_id]
            class_name = class_names.get(class_id, f"Unknown-{class_id}")

            print(f"  {class_name}: {ann_count}ê°œ ì–´ë…¸í…Œì´ì…˜, {img_count}ê°œ ì´ë¯¸ì§€")

            if ann_count < 30:  # 30ê°œ ë¯¸ë§Œì„ ì†Œìˆ˜ í´ë˜ìŠ¤ë¡œ ì •ì˜
                minority_classes.append((class_id, class_name, ann_count))
            elif ann_count > 100:  # 100ê°œ ì´ˆê³¼ë¥¼ ë‹¤ìˆ˜ í´ë˜ìŠ¤ë¡œ ì •ì˜
                majority_classes.append((class_id, class_name, ann_count))

        print(f"\në¶ˆê· í˜• ë¶„ì„:")
        print(f"  ì†Œìˆ˜ í´ë˜ìŠ¤ (<30ê°œ): {len(minority_classes)}ê°œ")
        print(f"  ë‹¤ìˆ˜ í´ë˜ìŠ¤ (>100ê°œ): {len(majority_classes)}ê°œ")
        print(f"  ë¶ˆê· í˜• ë¹„ìœ¨: {max(class_annotation_count.values()) / min(class_annotation_count.values()):.1f}ë°°")

        return minority_classes, majority_classes, class_annotation_count

"""# ëª¨ë¸ í•™ìŠµ"""

class RTDETRTrainer:
    def __init__(self, data_yaml_path):
        self.data_yaml_path = data_yaml_path
        self.model = None

    def train(self, epochs=100, imgsz=640, batch=16):
        """ê¸°ë³¸ RT-DETR ëª¨ë¸ í•™ìŠµ"""
        print("RT-DETR ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
        self.model = RTDETR('rtdetr-l.pt')

        results = self.model.train(
            data=self.data_yaml_path,
            epochs=epochs,
            imgsz=imgsz,
            batch=batch,
            name='pill_rtdetr',
            save_period=10,
            patience=15,       # ì¡°ê¸° ì¢…ë£Œ ì™„í™”
            cls=1.0,           # í´ë˜ìŠ¤ ì†ì‹¤ ê°€ì¤‘ì¹˜ ì¦ê°€
            box=7.5,           # ë°•ìŠ¤ ì†ì‹¤ ìœ ì§€
            dfl=1.5,              # Distribution focal loss (ê¸°ë³¸ 1.5)

            # í•™ìŠµë¥  ì¡°ì •
            lr0=0.01,          # ê¸°ë³¸ê°’
            lrf=0.01,
            momentum=0.937,
            weight_decay=0.0005,

            # ì •ê·œí™” ê°•í™”
            dropout=0.1,       # ë“œë¡­ì•„ì›ƒ ì¶”ê°€

            # ë°ê¸° ë¶„í¬ (95-165) ê³ ë ¤í•œ ì¦ê°•
            hsv_v=0.2,         # ë°ê¸° ë³€í™” ì œí•œ (ê¸°ì¡´ë³´ë‹¤ ì‘ê²Œ)
            hsv_s=0.3,         # ì±„ë„ ë³€í™” ì ì ˆíˆ
            hsv_h=0.01,        # ìƒ‰ì¡°ëŠ” ìµœì†Œí™”

            # RGB ìƒê´€ê´€ê³„ ê³ ë ¤
            mixup=0.0,         # ìƒ‰ìƒ í˜¼í•© ìµœì†Œí™”
            mosaic=0.3,        # ëª¨ìì´í¬ë„ ì¤„ì„
        )

        print("í•™ìŠµ ì™„ë£Œ!")
        return results

    def load_best_model(self, model_path=None):
        """ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ë¡œë“œ"""
        if model_path is None:
            model_path = 'runs/detect/pill_rtdetr/weights/best.pt'  # Phase 3 ê²½ë¡œ

        self.model = RTDETR(model_path)
        return self.model

class RTDETRInference:
    def __init__(self, model_path, mapping_file_path=None):
        self.model = RTDETR(model_path)

        # ğŸ”¥ ë§¤í•‘ íŒŒì¼ ë¡œë“œ (YOLO í´ë˜ìŠ¤ â†’ dl_idx)
        self.class_to_dl_idx = {}
        if mapping_file_path and os.path.exists(mapping_file_path):
            with open(mapping_file_path, 'r', encoding='utf-8') as f:
                self.class_to_dl_idx = json.load(f)
                # ë¬¸ìì—´ í‚¤ë¥¼ ì •ìˆ˜ë¡œ ë³€í™˜
                self.class_to_dl_idx = {int(k): int(v) for k, v in self.class_to_dl_idx.items()}
            print(f"âœ… ë§¤í•‘ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(self.class_to_dl_idx)}ê°œ í´ë˜ìŠ¤")
        else:
            print("âš ï¸ ë§¤í•‘ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. í´ë˜ìŠ¤ IDë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.")

    def predict_and_generate_csv(self, test_images_path, output_csv_path, conf_thresh=0.321):
        """í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ ì˜ˆì¸¡ ë° ì œì¶œ ê·œì¹™ì— ë§ëŠ” CSV íŒŒì¼ ìƒì„±"""
        results_data = []
        annotation_id = 1  # 1ë¶€í„° ì‹œì‘

        # í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡
        test_files = glob.glob(os.path.join(test_images_path, "*.png")) + \
                    glob.glob(os.path.join(test_images_path, "*.jpg"))

        print(f"í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€ {len(test_files)}ê°œ ì²˜ë¦¬ ì¤‘...")

        for img_path in test_files:
            filename = os.path.basename(img_path)

            # ğŸ”¥ image_id: íŒŒì¼ëª…ì—ì„œ ìˆ«ìë§Œ ì¶”ì¶œ (100.png â†’ 100)
            image_id = filename.split('.')[0]  # í™•ì¥ì ì œê±°
            try:
                image_id = int(image_id)  # ìˆ«ìë¡œ ë³€í™˜
            except ValueError:
                # ìˆ«ìê°€ ì•„ë‹Œ íŒŒì¼ëª…ì˜ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                image_id = filename.split('.')[0]

            # ì˜ˆì¸¡ ìˆ˜í–‰
            results = self.model(img_path, conf=conf_thresh)

            for result in results:
                boxes = result.boxes
                if boxes is not None:
                    for box in boxes:
                        # ë°”ìš´ë”© ë°•ìŠ¤ ì¢Œí‘œ (xyxy í˜•ì‹)
                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()
                        confidence = box.conf[0].cpu().numpy()
                        yolo_class_id = int(box.cls[0].cpu().numpy())  # YOLO í´ë˜ìŠ¤ ID

                        # ğŸ”¥ category_id: YOLO í´ë˜ìŠ¤ â†’ dl_idx ë³€í™˜
                        if yolo_class_id in self.class_to_dl_idx:
                            category_id = self.class_to_dl_idx[yolo_class_id]  # ì‹¤ì œ dl_idx
                        else:
                            category_id = yolo_class_id  # ë§¤í•‘ ì‹¤íŒ¨ì‹œ ê·¸ëŒ€ë¡œ
                            print(f"âš ï¸ ë§¤í•‘ ì—†ìŒ: YOLO í´ë˜ìŠ¤ {yolo_class_id} â†’ {yolo_class_id}")

                        # COCO í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (xywh)
                        bbox_x = x1
                        bbox_y = y1
                        bbox_w = x2 - x1
                        bbox_h = y2 - y1

                        results_data.append({
                            'annotation_id': annotation_id,  # 1ë¶€í„° ì‹œì‘
                            'image_id': image_id,            # íŒŒì¼ëª…ì—ì„œ ìˆ«ì ì¶”ì¶œ
                            'category_id': category_id,      # ì‹¤ì œ dl_idx
                            'bbox_x': bbox_x,
                            'bbox_y': bbox_y,
                            'bbox_w': bbox_w,
                            'bbox_h': bbox_h,
                            'score': confidence
                        })

                        annotation_id += 1

        # CSV íŒŒì¼ ìƒì„±
        df = pd.DataFrame(results_data)
        df.to_csv(output_csv_path, index=False)

        # ğŸ”¥ ê²°ê³¼ ë¶„ì„ ì¶œë ¥
        if len(results_data) > 0:
            unique_image_ids = df['image_id'].unique()
            unique_category_ids = df['category_id'].unique()

            print(f"\nğŸ¯ ì œì¶œ íŒŒì¼ ìƒì„± ì™„ë£Œ: {output_csv_path}")
            print(f"  - ì´ ì˜ˆì¸¡: {len(results_data)}ê°œ")
            print(f"  - ì´ë¯¸ì§€ ìˆ˜: {len(unique_image_ids)}ê°œ")
            print(f"  - ì¹´í…Œê³ ë¦¬ ìˆ˜: {len(unique_category_ids)}ê°œ")
            print(f"  - image_id ë²”ìœ„: {min(unique_image_ids)} ~ {max(unique_image_ids)}")
            print(f"  - category_id (dl_idx): {sorted(unique_category_ids)}")
        else:
            print("âš ï¸ ì˜ˆì¸¡ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ì‹ ë¢°ë„ ì„ê³„ê°’ì„ ë‚®ì¶°ë³´ì„¸ìš”.")

        return df

"""# ë©”ì¸ ì‹¤í–‰"""

def run_detailed_pipeline(data_path="./data"):
    """ìƒì„¸ ë²„ì „ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í´ë˜ìŠ¤ ê¸°ë°˜)"""
    print("=== ìƒì„¸ ë²„ì „ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

    # 1. ë°ì´í„° ì „ì²˜ë¦¬
    print("1. ë°ì´í„° ì „ì²˜ë¦¬...")
    # ì „ì²˜ë¦¬ ì‹¤í–‰
    processor = RTDETRDataProcessor(data_path)
    merged_data = processor.process_data_with_name_mapping()

    # ë§¤í•‘ ê²°ê³¼ í™•ì¸
    processor.validate_category_mapping()

    2. ëª¨ë¸ í•™ìŠµ
    print("2. ëª¨ë¸ í•™ìŠµ...")
    trainer = RTDETRTrainer(f"{data_path}/yolo_format/dataset.yaml")
    trainer.train(epochs=50, batch=8)

    # 3. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±
    print("3. ì¶”ë¡  ë° ì œì¶œ íŒŒì¼ ìƒì„±...")
    model_path = 'runs/detect/pill_rtdetr4/weights/best.pt'
    mapping_file = f"{data_path}/yolo_format/category_id_mapping.json"  # ğŸ”¥ ë§¤í•‘ íŒŒì¼ ê²½ë¡œ

    inference = RTDETRInference(model_path, mapping_file)

    submission_df = inference.predict_and_generate_csv(
        test_images_path=f"{data_path}/test_images",
        output_csv_path="submission_detailed_0.321.csv"
    )

    print("ìƒì„¸ ë²„ì „ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    return submission_df

data_path="./data"
run_detailed_pipeline(data_path)

